# 第１章
## DataFrame
`DataFrame`の主な利点の一つは、Sparkエンジンがまず論理実行プランを構築し、そのあとにコストオプティマイザによって決定された物理プランに基づいて生成されたコードを実行すること  
DataFrameが登場したことによって全ての言語でパフォーマンスが等しくなった
## DataSetについて
`type DataFrame = DataSet[Row]`とDataFrameはDatasetの一種として定義されていて、DataSetは任意の型パラメータを取ることができる
```
case class Person(name: String,height: Int)
val people: Dataset[Person]
```
## Catalyst Optimizer
- SparkSQLの中核にあるのがCatalyst Optimizer
- このオプティマイザは関数型プログラミングの構成要素を基盤としており、`新しい最適化の手法や機能をSparkSQLに追加しやすくすること`と、`外部の開発者がオプティマイザを拡張できるようにすること`
## Project Tungsten
TungstenはApacheSparkの実行エンジンの包括的なプロジェクトのコードネーム。Sparkのアルゴリズムを改善し、メモリやCPUの利用効率を高め、現代的なハードウェアのパフォーマンスを限界近くまで使い切ること
- メモリを直接管理、オーバーヘッドをなくす
- メモリ階層を活用するアルゴリズムとデータ構造の設計
- 実行時にコードを生成することによって、アプリケーションが現代的なコンパイラやCPUのための最適化を活用できるようにすること
- 仮想関数ディスパッチをなくすことによって複数のCPUの呼び出しを減らすこと
- 低レベルのプログラミングを利用し、メモリアクセスを高速化、Sparkのエンジンが単純なループを効率的にコンパイルして実行できるよう最適化を行ったりする
## DatasetとDataFrameの統合
Dataset APIにはsum(),avg(),join(),group()といった高レベルのドメイン固有言語の操作が含まれている。この特徴が意味しているのはこれまでのSpark RDDの柔軟性を保ちながらコードの表現力や読み書きのしやすさが高まるということ。  
Datasetは式やデータフィールドをクエリプランナーに渡し、Tungstenの高速なインメモリエンコーディングを利用することによってCatalystOptimizerの利点を活かせる。
## SparkSession
データの読み取り、メタデータの処理、セッションの設定、クラスタのリソース管理のためのエントリーポイント
## Tungstenフェーズ２
以下にフォーカス
- メモリ管理とバイナリ処理  
アプリケーションが受け持つ範囲を広げてメモリを明示的に管理するようにし、JVMのオブジェクトモデルとガベージコレクションのオーバヘッドをなくす
- キャッシュを意識した演算処理  
メモリの階層を活用するようなアルゴリズムおよびデータ構造
- コード生成  
コード生成によって現代的なコンパイラやCPUを利用する
# 第２章 耐障害性分散データセット
## RDDの生成
データのフォーマットも、テキスト、parquet、JSON、Hiveのテーブルといった複数のフォーマットがサポートされており、リレーショナルデータベースからJDBCドライバを使ってデータを読み取ることができる。ファイルから読み取られたデータは`MapPartitionsRDD`コレクションを.paralellize(...)とした場合は`ParallelCollectionRDD`になる。
## スキーマ
RDDはスキーマレスなデータ構造。tuple,dict,listなどほとんどあらゆるものを混在させることが可能である。  
`.collect()`メソッドはRDD内の全ての要素をリストとしてシリアライズし、ドライバに返す
## グローバルとローカルのスコープ
Sparkは本来的に並列性を持っている。クラスタモードではジョブが投入されて実行される時、そのジョブはドライバのーどに送られる。ドライバのーどはそのジョブのDAGを生成し、それぞれのタスクを実行するエクゼキュータノードを決定する。
## .flatMap(...)変換
リストではなくフラット化された結果を返す。
## .distinct(...)変換
指定された列中のユニークな値のリストを返す
## .sample(...)変換
データセットからランダムにサンプリングした結果を返す。
### 第１引数：サンプリングで置き換えを許すかどうかの指定(false)
### 第２引数：返すデータの比率(0.1)
### 第３引数：擬似乱数生成器に渡すシード(666)
## .repartition(...)変換
データセットのパーティション分割のやり直し
### .glom()メソッド
特定のパーティション中にあるデータセットの全ての要素の`リストを要素とするリストを生成`
## .take(...)メソッド
takesampleメソッド　ランダムに選択されたレコードを取得
## .reduce(...)メソッド
指定されたメソッドを使ってRDDの要素に対しreduceの処理を行う。かくパーティションで合計を取るためのメソッドを実行し、最終的な集計が行われるドライバノード
に合計を返している。
### .reduceByKey(...)
reduceの処理をキー単位で行う
## .foreach(...)メソッド
同じ関数をRDDの各要素に適用していく。指定された関数を各レコードに１つずつ適用していく。
## まとめ
ScalaのRDDとPythonのRDDの大きな違いの一つは速度。PythonのRDDの処理は、Scalaでの同じ処理に比べてはるかに低速になる。
# 第３章　DataFrame
DataFrameはApache Spark1.0においてSchemaRDDという名前でExperimentalな機能として導入され、ApacheSpark1.3リリースでDataFrameという名前に変更。データを構造化することでApacheSparkのエンジン、中でもCatalyst OptimizerはSparkのクエリパフォーマンスを大きく改善できる。
## PythonからRDDへの通信
ジョブの実行に際して潜在的に大きなオーバーヘッドが生ずる可能性がある。
- SparkContextがPy4Jを使ってJVMを起動
- RDDに対する変換は、まずJavabのPythonRDDオブジェクトに対してマッピングされる。
- これらのタスクがSparkのワーカーに送信されるとワーカーはPythonのSubprocessを起動し、コードとデータをパイプで送信してPythonで処理させる。
## Catalystオプティマイザ再び
高速な主な理由の１つはCatalyst Optimizer。RDBMSの論理・物理プランナーおよびコストモデル・コストベースの最適化に似ている
オプティマイザは関数型プログラミングの構成要素を基盤としており、「新しい最適化の手法や機能をSparkSQLに追加しやすくすること」「外部の開発者がオプティマイザを拡張できるようにする」ことを念頭において設計
## DataFrameによるPysparkの高速化
DataFrameとCatalystOptimizerが際立っているのは最適化されていないRDDのクエリに比べてPysparkのクエリのパフォーマンスを向上させてくれること
## 一時テーブルの生成
spark.read.jsonによってRDDからDataFrameへの変換が行われる際に実行。
mapやmapPartitionsという操作がDatFrame生成に必要な操作である。
行なっているのがDataFrameの操作であっても、その操作をデバッグする際にはそれがSparkUI場ではRDDの操作であることを覚えておく。
## DataFrameAPIでのクエリ
collect()はDataFrame中の全ての行を返すので、全ての行がエグゼキュータからドライバへ戻されることになる。
## RDDとのやりとり
### 既存のRDDをDataFrameに変換する方法
- リフレクションを使ってスキーマを推測させる方法
- プログラムからスキーマを指定する方法
## リフレクションによるスキーマの推定
行のオブジェクトはキー/値のペアのリストを**kwargsという行のクラスに渡すことによって構築
## プログラムからのスキーマの死t例
### StrictFieldクラス
- name:フィールドの名前
- dataType:フィールドのデータ型
- nullable:nullになり得るか
## まとめ
ScalaのDataFrameのラッパーが用意されたことによってDataFrameを使えばPythonユーザーはPythonのサブプロセス/JVM間のコミュニケーションのオーバーヘッドを回避できるようになった。
# 第４章　データモデリングの準備
## 重複
### 完全一致な重複を排除する
`df = df.dropDuplicates()`
### ID以外
```
df = df.dropDuplicates(subset=[
    c for c in df.columns if c != 'id'
])
```
### age(...)メソッドを使って一度に計算
## 計測値の欠落
### .dropna(...)メソッド
データを削除。行を削除する閾値を決めるthreshパラメータも用いることができる
### .fillna(...)メソッド
欠落値を補う。
##　外れ値
### .approxQuantile(...)メソッド
- １つ目のパラメータ：列名
- ２つ目のパラメータ：０から１の間の数値を指定するかリストを指定できる。
- ３つ目のパラメータ：計測値における誤差の許容レベルを指定
## 記述統計
データセット関する基本的な情報を示す。
### .describe()、絵ソッド
数値型の列
## ヒストグラム
### ヒストグラムの生成方法
- ワーカーでデータを集計し、集計されたヒストグラムの瓶のリストをドライバに返させ、それぞれの瓶をカウントする
- 全てのデータポイントをドライバに返させ、プロットを行うライブラリのメソッドにあとは任せる
- データをサンプリングしてからドライバに返させ、プロットを行う。
BokehはDS.jsを基盤としているので、生成されるグラフはインタラクティブなものになっている。
